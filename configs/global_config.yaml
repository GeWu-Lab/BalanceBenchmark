seed: 42

dataset:
  dataloader:
    eff_batch_size: 1024 # effective batch_size = num_node*num_process_per_node*batch_size_per_process = world_size*batch_size_per_process
    batch_size: -1 # batch size per process, set to -1 to use eff_batch_size
    num_workers: 4 # number of workers for dataloader
    fast_run: false # if true, only use part of the dataset
    shuffle: true # if true, shuffle the dataset
    drop_last: true # if true, drop the last batch if the size is not equal to batch_size
    pin_memory: true # if true, use pin_memory for dataloader

fabric:
  accelerator: "gpu" # "cpu", "gpu", "cuda", "mps", "tpu"
  devices: [0] # number of devices or list of index
  precision: "32-true" # "32-true", "16-mixed", "bf16-mixed", etc.
  strategy: "dp" # "dp", "ddp", "ddp2", "ddp_spawn"

log:
  logger_name: ['tensorboard'] # list of logger name
  wandb_name: ''
  log_interval: 100 # how many batches to wait before logging training status
  log_per_epoch: -1 # number of log for each epoch. If set will override log_interval
  comment: '' # comment for the logger

model:
  type: VTClassifier
  n_classes: 101
  fusion: concat

train:
  parameter:
    total_epoch: 70 # number of epochs to train
    warmup: 10 # warmup beta and lr
    base_lr: 0.001 # base learning rate. Using linear law, lr = eff_batch_size/base_bs*base_lr
    lr: -1 # learning rate. If set, override base_lr, set to -1 to use base_lr
    base_batch_size: 2 # base batch_size
    lr_scaling_rule: "linear" # "sqrt", "linear" learning rate should scale with batch size
  checkpoint:
    resume: '' # path to the checkpoint
    checkpoint_frequency: 1 # number of epoch interval to save the checkpoint
    num_checkpoint_keep: 3 # set to -1 to save all checkpoints
  optimizer:
    type: "SGD" # type of optimizer
    momentum: 0.9
    weight_decay: 0.0001
  scheduler:
    type: "StepLR" # type of scheduler
    step_size: 30 # step size for StepLR
    gamma: 0.1 # gamma for StepLR
  validation:
    frequency: 1 # number of epoch interval to do validation
    select_best: 'last' # 'last', 'best' select the best model based on validation loss
  loss:
    type: "CrossEntropyLoss" # type of loss
trainer:  
  trainer: "baselineTrainer"
  name: 'baseline'
  gpus: "0"
trainer_para:
  base:
    grad_accum_steps: 1
    max_epochs: 70
  baseline:
    alpha: 0.3
    method: 'None'
    modulation_starts: 10
    modulation_ends: 80
    modality: 2
  OGM:
    alpha: 0.3
    method: 'OGM_GE'
    modulation_starts: 10
    modulation_ends: 80
  AGM:
    alpha: 0.1
    method: "None"
    modulation_starts: 10
    modulation_ends: 80
    modality: 2
  AMCo:
    alpha: 0.1
    method: "None"
    modulation_starts: 10
    modulation_ends: 80
    sigma: 0.5
    U: 512
    eps: 0.3
    modality: 2
  CML:
    alpha: 0.1
    method: "None"
    modulation_starts: 10
    modulation_ends: 80
    lam: 0
    modality: 2
  GBlending:
    alpha: 0.1
    method: "None"
    modulation_starts: 0
    modulation_ends: 80
    super_epoch: 10
    modality: 2
  PMR:
    alpha: 0.1
    method: "None"
    modulation_starts: 0
    modulation_ends: 80
    modality: 2
    embed_dim: 512
    momentum_coef: 0.5


